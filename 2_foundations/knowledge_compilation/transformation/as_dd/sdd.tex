\subsubsection{Probabilistic Sentential Decision Diagrams (PSDD)}
\label{sec:psdd}

A \acrfull{psdd} is a tractable representation for a probability distribution over a set of propositional variables subject to logical constraints.  In essence, a \acrshort{psdd} is a parameterized \acrfull{sdd} in which each node is assigned a well-defined local distribution.  By construction, the \acrshort{psdd}’s global distribution respects a given \emph{base theory} (i.e., a propositional formula representing constraints), assigns zero probability to every assignment violating that theory, and factors the remaining assignments according to a hierarchical decomposition.

% this is incorrect, vtree defines \acrshort{sdd}, not in parallel
% To define a \acrshort{psdd} precisely, one must first choose:
% \begin{enumerate}
% \item A \emph{vtree}, which is a full binary tree whose leaves are in bijection with the set of variables.  The internal nodes of the vtree determine how variables are split and combined at different levels of the diagram.
% \item A \emph{normalized \acrshort{sdd}} encoding the same propositional constraints as the original theory.  An \acrshort{sdd} is built from \emph{decision nodes} and \emph{terminal} (leaf) nodes:
% \begin{itemize}
% \item A \emph{decision node} has elements of the form $(p_i, s_i)$, each comprising two $sub-$SDDs: a \emph{prime}$p_i$ and a \emph{sub}$s_i$.
% \item A \emph{terminal node} is simply a literal (e.g., $X$, $\neg X$) or a constant ($\top$ or $\bot$).
% \end{itemize}
% Normalization requires that each decision node’s primes refer to one vtree subtree and subs refer to a disjoint subtree, reflecting how variables are partitioned at each vtree node.
% \end{enumerate}

\begin{definition}[Normalized \acrshort{sdd}]
Given a \emph{vtree}, $v$, over variables ${X_1, ... , X_n}$, an normalized \acrshort{sdd} over $T$ is defined recursively as follows:
\begin{enumerate}
    \item A \emph{terminal} node is a literal $X_i$ or $\neg X_i$
\item At an \emph{internal vtree node} with left subtree $V_l$ and right subtree $V_r$, a normalized \acrshort{sdd} is a finite disjunction: $$\bigvee_{i=1}^k \left( P_i \land S_i \right)$$ where:
\begin{enumerate}
    \item For every $i$,  $P_i$ is a normalized \acrshort{sdd} over the variables in $V_l$, and $S_i$ is a normalized \acrshort{sdd} over the variables in $V_l$.
    \item The set $\{P_1, \ldots, P_k\}$ forms a partition of the space over $V_l$; that is, $\forall i \neq j: P_i \land P_j = \bot$ (mutually exclusive) and $\bigvee_{i=1}^k P_i = \top$(exhaustive).
    \item Each $S_i$ is distinct (no two are equivalent).
    \item Each prime-sub pair $(P_i, S_i)$ is compressed: distinct primes never associate to equivalent subs and vice versa (no redundancy).
\end{enumerate}

\end{enumerate}
\end{definition}

Once the normalized \acrshort{sdd} is fixed, one may introduce continuous parameters to obtain a \acrshort{psdd}. These parameters, in effect, turn each decision node into a \emph{local mixture} of its prime components, while terminal nodes over variables become Bernoulli distributions.

\begin{definition}[\acrshort{psdd} Syntax]
\label{def:psdd-syntax}
Let $n$ be an \acrshort{sdd} node normalized for a vtree node $v$.
\begin{itemize}
\item If $n$ is a terminal node:
\begin{enumerate}
\item If it encodes a literal (e.g.\ $X$, $\neg X$) or the constant~$\bot$, then its probability is fixed implicitly (e.g.\ $\neg X$ yields $\Pr(\neg X)=1,$ $\Pr(X)=0$ for that node alone).
\item If it is the constant~$\top$ and $v$ corresponds to some variable $X$, then we assign a parameter $\theta\in(0,1)$ indicating $\Pr(X)$ at this node.
\end{enumerate}
\item If $n$ is a decision node with $k$ elements $[(p_1,s_1), \ldots, (p_k,s_k)]$, we assign nonnegative parameters $\theta_1,\ldots,\theta_k$ such that $\sum_{i=1}^k \theta_i=1$.  Furthermore, if $s_i = \bot$, then $\theta_i$ must be zero (no probability is allotted to a sub whose base is unsatisfiable).
\end{itemize}
The resulting parameterized structure is called a \emph{\acrshort{psdd}}.
\end{definition}

Each node $n$ in a \acrshort{psdd} induces a local distribution $\Pr_n(\cdot)$ on the variables of the vtree node $n$ is normalized for.  At a decision node, the probability of a complete assignment is given by multiplying:
\begin{enumerate}
\item The probability that we "choose" a particular prime~$p_i$, labeled by $\theta_i$.
\item The probability contributed by prime node~$p_i$ on its variables.
\item The probability contributed by sub node~$s_i$ on its (disjoint) variables.
\end{enumerate}
Summing across all primes yields $\Pr_n$ at that node.  By construction, the \emph{root} node’s distribution~$\Pr_r$ then covers all variables and zeroes out any assignments that do not satisfy the base theory.

\begin{theorem}[Base Property \cite{darwiche_knowledge_2002}]
\label{thm:psdd-base}
If a \acrshort{psdd} node $n$ is normalized for vtree node~$v$, then $\Pr_n(x)=0$ whenever $x$ does not satisfy the \acrshort{sdd} sub-formula~$[n]$.  At the root node $r$, $\Pr_r(x)>0$ only if $x$ satisfies the entire theory.
\end{theorem}

\paragraph{Parameter Semantics.} A key property of \acrshort{psdd}s is that each parameter $\theta_i$ can be interpreted \emph{locally} as a conditional probability given the \emph{context} of the decision node.  Formally, if node~$n$ has context~$\gamma_n$ (i.e., the partial assignment implied by traversing the \acrshort{sdd} from the root to~$n$), then:
\[
\theta_i =\Pr\bigl([p_i] \bigm|[\gamma_n]\bigr)
\]
where $[,p_i]$ is the logical content of prime $p_i$.  This ensures that local parameters align with global $\Pr(\cdot)$ in a transparent, compositional way.

\paragraph{Context-Specific Independence.} Due to the vtree-based factorization, \acrshort{psdd}s capture rich \emph{context-specific independences} \cite{boutilier_context-specific_2013}.  At high level, once we know the node’s context (which is a partial assignment or formula), certain subsets of variables become conditionally independent of the rest.  These independence statements can be read directly from the \acrshort{psdd} structure, generalizing common conditional-independence ideas in Bayesian or Markov networks.

\paragraph{Inference and Tractability.} A central advantage of \acrshort{psdd}s is that computing $(\Pr_r(e)$ for any evidence~$e$ can be done in time linear in the size of the \acrshort{psdd}.  This incremental algorithm proceeds bottom-up through each node, locally aggregating evidence contributions and summing accordingly.  Moreover, once node-level evidence statistics are available, one can also efficiently compute single-variable or pairwise marginals using a second top-down pass.

\paragraph{Parameter Learning under Complete Data.} Another appealing property is that \emph{maximum-likelihood} parameters can be determined in closed form when every training example is a complete assignment of all variables.  Specifically, if a \acrshort{psdd} node $n$ with context~$\gamma_n$ has elements $(p_i, s_i)$, one sets
\[
\theta_i = \frac{\text{number of data points satisfying both } \gamma_n \text{ and } p_i}
{\text{number of data points satisfying } \gamma_n}.
\]
Parallel rules apply for terminal nodes representing $\top$.  Because sub-contexts $\gamma_n \wedge p_i$ are pairwise disjoint, it suffices to tabulate data counts for each feasible sub-context.  The outcome is a simple frequency-based update analogous to parameter estimation in Bayesian networks, yet here it respects the underlying \acrshort{sdd} constraints exactly.

For any propositional distribution (and chosen vtree), there exists a corresponding \acrshort{psdd} whose root distribution matches it exactly.  Furthermore, if the \acrshort{psdd} is kept \emph{compressed} (meaning no redundant substructures), this representation is \emph{unique} up to isomorphic details \cite{darwiche_knowledge_2002}.  Thus, \acrshort{psdd}s can serve as canonical forms for distributions under logical constraints.

In contrast to classical graphical models, \acrshort{psdd}s operate at the confluence of tractable \emph{Boolean} structure (via \acrshort{sdd}s) and probability theory.  They explicitly encode zero-probability assignments (via the \acrshort{sdd} base) while ensuring all positive assignments factor through the decision nodes.  Their parameter semantics aligns each local weight with a well-defined global conditional probability.  In addition, closed-form parameter learning is possible in the complete-data setting.  Hence, \acrshort{psdd}s provide a principled, canonical choice for modeling distributions when the domain is governed by complex logical constraints.