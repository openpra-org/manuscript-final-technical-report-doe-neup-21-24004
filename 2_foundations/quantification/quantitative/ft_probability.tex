\section{Fault Tree Probability Estimation}
\label{sec:fault_tree_probability_estimation}

Beyond describing which combinations of basic events can fail, most \acrfull{fta} requires a quantitative assessment of the \emph{likelihood} that the top event ultimately occurs. This section details how probabilities are embedded within the fault tree structure, how to compute the top eventâ€™s failure probability (or system \emph{unreliability}), and some common methods for handling large or dependent fault trees.

\subsection{Assigning Probabilities to Basic Events}

Let \(\mathcal{B}=\{b_1, \dots, b_n\}\) be the set of basic events in the fault tree \(F\).  Each basic event \(b\) is associated with a \emph{failure probability} \(p(b)\in [0,1]\).  Interpreted as a Boolean random variable \(X_b\), event \(b\) takes value \(1\) (failure) with probability \(p(b)\) and value \(0\) (success) with probability \(1-p(b)\).  Thus, 
\[
\Pr\bigl[X_b = 1\bigr] \;=\; p(b), 
\quad
\Pr\bigl[X_b = 0\bigr] \;=\; 1-p(b).
\]

In the simplest \emph{single-time} analysis, each basic event is either failed or functioning for the entire time horizon under study, and no component recovers once it has failed.

\subsection{Top Event Probability Under Independence}

If we assume that all basic events fail independently, then for any set \(S \subseteq \mathcal{B}\) of failed basic events,
\[
\Pr\bigl[\text{basic events in }S\text{ fail and others succeed}\bigr]
\;=\;
\prod_{b \in S} p(b)\,\times\!\!\prod_{b \notin S} [1 - p(b)].
\]
Recall from Section~\ref{sec:fault_tree_definition} that the top event \(T\) (also called \(\mathrm{TE}\)) fails given \(S\) precisely if \(\pi_F(S, T)=1\).  Hence, the probability that the top event fails is the sum of these independent configurations \(S\) for which \(\pi_F(S,T)=1\):
\begin{equation}
\label{eq:top_event_probability}
\Pr\bigl[X_t=1\bigr]
\;=\;
\sum_{S\,\subseteq\,\mathcal{B}}
\Bigl[
    \pi_F(S,T)
    \prod_{b \in S} p(b)
    \prod_{b \notin S} \bigl[1 - p(b)\bigr]
\Bigr].
\end{equation}
A direct computation of \eqref{eq:top_event_probability} often becomes unwieldy for large FTs because of the exponential number of subsets \(S\).  However, if the fault tree is \emph{simple} (no shared subtrees) and each gate in \(\mathcal{G}\) has independent inputs, one may propagate probabilities \emph{bottom-up} through the DAG using basic probability rules:
\[
\begin{aligned}
\Pr[g=1] \;=\;& \prod_{x \in I(g)} \Pr[x=1],
&&\text{if gate \(g\) is AND,}\\[6pt]
\Pr[g=1] \;=\;& 1 \;-\; \prod_{x \in I(g)}\bigl[1-\Pr[x=1]\bigr],
&&\text{if gate \(g\) is OR,}\\[6pt]
\Pr[g=1] \;=\;& 1 \;-\; \Pr[x=1],
&&\text{if gate \(g\) is NOT (single input \(x\)),}\\[6pt]
\Pr[g=1] \;=\;& \displaystyle \sum_{j=k}^{\,|I(g)|}
\;\;\sum_{\substack{A\,\subseteq\,I(g)\\|A|=j}}
\;\;\prod_{x\in A}\Pr[x=1]\;\prod_{x\in I(g)\setminus A}\bigl[1-\Pr[x=1]\bigr],
&&\text{if gate \(g\) is \(\mathrm{VOT}(k/n)\),}\\[6pt]
\Pr[g=1] \;=\;& \displaystyle \sum_{\substack{A\,\subseteq\,I(g)\\\text{\(|A|\) is odd}}}
\;\;\prod_{x\in A}\Pr[x=1]\;\prod_{x\in I(g)\setminus A}\bigl[1-\Pr[x=1]\bigr],
&&\text{if gate \(g\) is XOR.}
\end{aligned}
\]

\subsection{Handling Dependent Failures}

In many real-world fault trees, a single basic event \(b\) may feed into multiple gates, thereby making different branches of the tree dependent on one another. This violates the independence assumptions that simple bottom-up probability propagation requires. Below, we list several techniques that address or approximate these dependencies:

\begin{enumerate}

  \item \textbf{Binary Decision Diagrams (BDDs).}  
  Using BDDs involves encoding the entire Boolean structure of the fault tree into a single, canonical DAG.  Sub-expressions within this diagram are cached, allowing an efficient evaluation of \(\Pr[X_t=1]\) even when basic events are shared across subtrees.  In many cases, BDDs compress large FTs and facilitate fast reliability inference.

  \item \textbf{Minimal Cut Sets (MCSs) and Approximations.}  
  Many approaches characterize the top event in terms of its minimal cut sets.  Let \(\{\mathrm{MCS}\}\) be the family of all minimal cut sets for the top event \(T\).  Each MCS \(C\subseteq \mathcal{B}\) is a smallest set of basic events whose simultaneous failure brings down the system.  

  \begin{itemize}
    \item \emph{Rare-Event Approximation.}  
    When each \(p(b)\) is small, the probability of two or more cut sets overlapping in their failures can be negligible.  A common approximation is to sum the probabilities of each MCS as if they were mutually exclusive:
    \begin{equation}\label{eq:rare_event_approx}
      \Pr[X_t=1]
      \;\approx\;
      \sum_{C\in \{\mathrm{MCS}\}}
      \;\prod_{b \in C} p(b).
    \end{equation}
    In highly reliable systems, this often yields a reasonable estimate.

    \item \emph{Min-Cut Upper Bound (MCUB).}  
    A related bounding technique interprets the top event as the union of all MCS failures. By the union bound,
    \begin{equation}\label{eq:mcub}
      \Pr[X_t=1]
      \;\le\;
      \sum_{C\in \{\mathrm{MCS}\}}
      \;\prod_{b \in C} p(b).
    \end{equation}
    Known as the \emph{min-cut upper bound}, this provides a guaranteed upper estimate for the probability of system failure.  However, if multiple MCSs share common basic events of non-negligible failure probability, one might overestimate \(\Pr[X_t=1]\).  
  \end{itemize}

  \item \textbf{Simulation-Based Methods.}  
  Monte Carlo simulation bypasses many analytical complexities by directly sampling the failure state of each basic event from its distribution \(p(b)\).  After sampling, one evaluates the fault tree deterministically (i.e., checks whether \(X_t=1\)).  Repeating over many samples yields an empirical estimate of the top event probability.  This approach is flexible, handles various dependencies, and easily extends to time-dependent or multi-state analyses.  However, for extremely rare failures, simulation can require large sample sizes to achieve low-variance estimates.

\end{enumerate}

Each of these techniques accommodates shared subtrees more effectively than naive bottom-up methods.  In practice, analysts often deploy a combination of them (e.g., using BDDs for certain gates, applying an MCS approximation for others) to balance accuracy, computational cost, and modeling complexity.