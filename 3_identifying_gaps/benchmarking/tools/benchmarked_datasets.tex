\section{Benchmarked Datasets}
\label{sec:benchmarked-datasets}

% table showing each run -> {benchmarked tools, datasets, environment for benchmark}
% showing what was benchmarked.
% https://github.com/openpra-org/synthetic-benchmarks
% table rows: datasets, columns: tools, cells: runs, environment.

This section provides an overview of the datasets used for profiling and benchmarking the PRA tools, the types of analyses performed, and the computational environments employed. The benchmarking campaign utilized a set of synthetic datasets, each designed to test specific aspects of tool performance.

The \texttt{500\_to\_750} dataset was specifically used for profiling the SCRAM tool. This dataset contains models with varying numbers of basic events (500, 550, 600, 750), different gate weights, and configurations with and without common cause failure (CCF) groups. Profiling with this dataset enabled detailed analysis of SCRAM's performance across probability quantification, importance measure calculation, uncertainty analysis (using 10,000 Monte Carlo samples), and CCF analysis. The profiling runs were conducted on a Dell Latitude E7470 laptop equipped with 8 GB RAM, providing insight into SCRAM's behavior on a resource-constrained platform.

All other datasets listed in Table~\ref{tab:syn_datasets_overview} were used for systematic benchmarking of the PRA tools. These include \texttt{100\_to\_100k\_voter}, \texttt{1\_to\_4k/saphsolve-jsinp}, \texttt{1\_to\_50k}, \texttt{1\_to\_5k}, and \texttt{2\_to\_100k}. Each dataset was constructed to explore different model sizes, gate configurations, and event probabilities, and was benchmarked using a subset of tools and algorithms as detailed in Table~\ref{tab:what_was_benchmarked}. The primary analyses performed were probability quantification, with additional analyses such as importance measures, uncertainty, and CCF analysis where supported.

It is important to note that the \texttt{c1-P\_0.01-0.05} through \texttt{c7-P\_0.35-0.9} datasets, although generated, were not included in the benchmarking campaign described here. These datasets are reserved for future studies or for analyses not covered in this work.

In addition to the main synthetic datasets, two special datasets were used: the \texttt{Non-CCF} and \texttt{CCF} model sets. The \texttt{Non-CCF} dataset comprises a large collection of models without common cause failure groups, while the \texttt{CCF} dataset includes models specifically designed to test CCF analysis capabilities. These datasets are not part of the main synthetic dataset list. A subset of inputs from the \texttt{Non-CCF} dataset was selected to perform a comprehensive benchmarking of all four tools: SCRAM, SAPHSOLVE, XFTA, and FTREX. The results of this benchmarking are presented in the next section.

The majority of benchmarking runs were performed on a dedicated server environment running Ubuntu 22.04.1 LTS (64-bit), with 1 CPU core, 63 GB RAM, and a 900 second time limit per run. This environment was used for all systematic benchmarking, ensuring consistency of results across tools and datasets.

\include{3_identifying_gaps/benchmarking/tables/benchmarked_datasets}

\begin{comment}
# Benchmark environment for profiling:
Hardware Model -- Dell Inc. Latitude E7470
Memory -- 8.0GiB
Processor -- Intel® Core™ i7-6600U CPU @ 2.60GHz × 4
Graphics -- Mesa Intel® HD Graphics 520 (SKL GT2)
Disk Capacity -- 256.1GB
OS Name -- Ubuntu 22.04.1 LTS
OS Type -- 64-bit

# Benchmark environment for all benchmarking other than profiling:
options:
resource limits:
- memory:                63000.0 MB
- time:                  900 s
- cpu cores:             1

# 100_to_100k_voter
RUN 1:
- Only SCRAM has been tested against this dataset. Only wall clock time has been recorded.

# 1_to_4k/saphsolve-jsinp
- SCRAM (MOCUS REA, MOCUS MCUB, BDD), XFTA, SAPHSOLVE have been benchmarked.
- Wall time, CPU time, and memory usage have been calculated.

# 1_to_50k

RUN 3:
- SAPHSOLVE and XFTA
- SAPHSOLVE (MOCUS MCUB) has been benchmarked for 2 truncation limits: 1E-14 and 1E-20.

# 1_to_5k

RUN 1:
- SAPHSOLVE and XFTA
- SCRAM (MOCUS REA, MOCUS MCUB, BDD), XFTA, SAPHSOLVE have been benchmarked.
- Wall time, CPU time, and memory usage have been calculated.

RUN 2:
- Only SAPHSOLVE
- Only CPU time and clock time

# 2_to_100k

RUN 2:
- SCRAM and XFTA have been tested against this dataset. For SCRAM - MOCUS, BDD, ZBDD have been utilized but in case of XFTA it is unknown which algorithm it used (MOCUS or BDD?).
-Only the wall clock time and CPU time have been calculated.

RUN 3:
- Both SCRAM and XFTA have been tested against this dataset. Wall time, CPU time, and memory usage have been calculated.

# 500_to_750
- Used for profiling SCRAM using scram.
- MOCUS, BDD, ZBDD algorithms have been tested.
- Probability analysis, importance measures, uncertainty analysis with 10,000 samples.
- Also, this dataset contained common cause failure groups. CCF analysis was done for SCRAM.
Here is an example CLI argument:
--probability --ccf --mocus --rare-event --importance --uncertainty --num-trials 10000

# c1-P_0.01-0.05


# c2-P_0.5-0.9/openpsa


# c3-P_0.01-0.9/openpsa


# c4-P_0.05-0.9/openpsa


# c5-P_0.1-0.9/openpsa


# c6-P_0.25-0.9/openpsa


# c7-P_0.35-0.9/openpsa

# unknown dataset
- Both non-CCF and CCF analysis have been benchmarked for SCRAM (MOCUS REA, MOCUS MCUB, BDD, ZBDD), XFTA, SAPHSOLVE. Here is the list of non-CCF models:
100-01
100-02
100-03
100-04
100-05
100-06
100-07
100-08
100-09
200-01
200-02
200-03
200-04
200-05
200-06
200-07
200-08
200-09
300-01
300-02
300-03
300-04
300-05
300-06
300-07
300-08
300-09
400-01
400-02
400-03
400-04
400-05
400-06
400-07
400-08
400-09
500-01
500-02
500-03
500-04
500-05
500-06
500-07
500-08
500-09
600-01
600-02
600-03
600-04
600-05
600-06
600-07
600-08
600-09
700-01
700-02
700-03
700-04
700-05
700-06
700-07
700-08
700-09
800-01
800-02
800-03
800-04
800-05
800-06
800-07
800-08
800-09
900-01
900-02
900-03
900-04
900-05
900-06
900-07
900-08
900-09
1000-01
1000-02
1000-03
1000-04
1000-05
1000-06
1000-07
1000-08
1000-09
2000-01
2000-02
2000-03
2000-04
2000-05
2000-06
2000-07
2000-08
2000-09
3000-01
3000-02
3000-03
3000-04
3000-05
3000-06
3000-07
3000-08
3000-09
4000-01
4000-02
4000-03
4000-04
4000-05
4000-06
4000-07
4000-08
4000-09
5000-01
5000-02
5000-03
5000-04
5000-05
5000-06
5000-07
5000-08
5000-09
6000-01
6000-02
6000-03
6000-04
6000-05
6000-06
6000-07
6000-08
6000-09
7000-01
7000-02
7000-03
7000-04
7000-05
7000-06
7000-07
7000-08
7000-09
8000-01
8000-02
8000-03
8000-04
8000-06
8000-07
8000-08
8000-09
9000-01
9000-02
9000-03
9000-04
9000-05
9000-06
9000-07
9000-08
9000-09
10000-01
10000-02
10000-03
10000-04
10000-05
10000-06
10000-07
10000-08
10000-09

Here is the list of CCF models:
ccf1-100-05
ccf2-100-05
ccf3-100-05
ccf4-100-05
ccf5-100-05
ccf6-100-05
ccf7-100-05
ccf8-100-05
ccf9-100-05
ccf1-200-05
ccf2-200-05
ccf3-200-05
ccf4-200-05
ccf5-200-05
ccf6-200-05
ccf7-200-05
ccf8-200-05
ccf9-200-05
ccf1-300-05
ccf2-300-05
ccf3-300-05
ccf4-300-05
ccf5-300-05
ccf6-300-05
ccf7-300-05
ccf8-300-05
ccf9-300-05
ccf1-400-05
ccf2-400-05
ccf3-400-05
ccf4-400-05
ccf5-400-05
ccf6-400-05
ccf7-400-05
ccf8-400-05
ccf9-400-05
ccf1-500-05
ccf2-500-05
ccf3-500-05
ccf4-500-05
ccf5-500-05
ccf6-500-05
ccf7-500-05
ccf8-500-05
ccf9-500-05
ccf1-600-05
ccf2-600-05
ccf3-600-05
ccf4-600-05
ccf5-600-05
ccf7-600-05
ccf8-600-05
ccf9-600-05
ccf1-700-05
ccf2-700-05
ccf3-700-05
ccf4-700-05
ccf5-700-05
ccf6-700-05
ccf7-700-05
ccf8-700-05
ccf9-700-05
ccf1-800-05
ccf2-800-05
ccf3-800-05
ccf4-800-05
ccf5-800-05
ccf6-800-05
ccf7-800-05
ccf8-800-05
ccf9-800-05
ccf1-900-05
ccf2-900-05
ccf3-900-05
ccf4-900-05
ccf5-900-05
ccf6-900-05
ccf7-900-05
ccf8-900-05
ccf9-900-05
ccf1-1000-05
ccf2-1000-05
ccf3-1000-05
ccf4-1000-05
ccf5-1000-05
ccf6-1000-05
ccf7-1000-05
ccf8-1000-05
ccf9-1000-05

- FTREX BDD and ZBDD (Name and ID) have been bechmarked agains the following models:
100-01
100-02
100-03
100-04
100-05
100-06
100-07
100-08
100-09
200-01
200-02
200-03
200-04
200-05
200-06
200-07
200-08
200-09
500_01
500_02
500_03
500_04
500_05
500_06
500_07
500_08
500_09
\end{comment}