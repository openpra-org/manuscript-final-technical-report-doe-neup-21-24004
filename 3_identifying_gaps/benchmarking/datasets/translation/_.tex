\section{Model Translation}
Model representation in the PRA domain is encumbered by format fragmentation and proprietary development practices. A small group of stakeholders, each using specialized file structures, has little incentive to converge on a single standard. Proposing a new "universal" format can inadvertently add yet another layer of fragmentation if adoption proves limited. Conversely, attempting a fully connected translation map among all existing formats is a significant undertaking, requiring both ongoing maintenance and broad community participation that may not materialize.

\subsection{PRAcciolini: Translation without a Single Intermediate Representation}
In view of these constraints, the short-term strategy is to create a system of lightweight interfaces under an open-source tool, referred to here as \emph{PRAcciolini}, that interlinks existing translation tools without defaulting to a single pivot format. Rather than striving for every possible conversion path, the goal is to form what might be called a "translation spanning tree": a subset of connections that covers most practical needs while avoiding excessive overhead. This design allows each domain-specific library to retain its native parsing and exporting routines, with conversions performed only when required.

\begin{figure}[h!]
  \includesvg[width=\textwidth]{3_identifying_gaps/benchmarking/datasets/translation/figs/translations-pracciolini.svg}
    \caption{Example of supported model translations in PRAcciolini.}
  \label{fig:translations}
\end{figure}

As shown in Figure~\ref{fig:translations}, a CAFTA XML file can be transformed into FTREX FTP or SAPHSOLVE JSON, while MAR-D data can be republished as OpenPSA XML and potentially mapped to a TensorFlow-compatible format. Round-trip testing (translating from one format to another and back again) verifies fidelity, ensuring that essential model information remains intact. By removing the need for a monolithic "canonical" structure, this partial translation network can remain both flexible and modular, minimizing risky lock-step updates whenever new releases or extensions emerge.

\subsection{Long-Term Goal: OpenPRA JSON Schema}
The long-term initiative extends beyond partial connectivity to propose an openly licensed JSON schema (OpenPRA JSON). This plan intends to balance retained compatibility with existing standards (e.g., OpenPSA XML) against the specialized requirements of nuclear licensing. Two major factors underlie this choice:

\begin{itemize}
\item \textbf{Extensibility and Clarity:} JSON natively supports hierarchical key-value pairs that facilitate both human readability and efficient parsing. PRA models tend to store domain-specific knowledge, not merely numeric parameters, making readability a high priority. Unlike FlatBuffers or similar purely binary formats, JSON offers a middle ground by allowing large, descriptive models without sacrificing parser performance.
\item \textbf{Nuclear-Specific Needs:} Although OpenPSA XML has served as a standard in probabilistic safety, it is no longer actively maintained. The OpenPRA JSON schema seeks to preserve core OpenPSA semantics while adding a namespace dedicated to nuclear regulation. This extension accommodates plant-specific elements and licensing-related fields that exceed the scope of general probabilistic models.
\end{itemize}

Thus, the near-term effort builds a manageable translation spanning tree through PRAcciolini, minimizing integration burdens across multiple closed-source tools. Simultaneously, OpenPRA JSON emerges as a robust, future-facing schema for those organizations requiring more comprehensive and domain-tailored data structures. This dual approach, maintaining workable interoperability while evolving a modern open standard, addresses both the immediate priorities of the PRA community and the long-term need for a sustainable, extensible foundation.

% A persistent challenge in PRA software development is the variety and fragmentation of data formats. In a domain with comparatively few stakeholders, introducing yet another universal format frequently aggravates fragmentation rather than alleviating it. At the same time, providing full interoperability among all existing formats is likewise complex, given both the number of competing syntaxes and limited incentives in predominantly closed development communities.

% In response, we are implementing an open framework that focuses on systematic translation rather than any single, newly imposed format. Central to this strategy is a flexible open-source schema, OpenPRA JSON, designed to accommodate content from external formats wherever possible. Since OpenPSA XML is widely recognized as the de facto industry standard, our immediate emphasis lies in translating to and from OpenPSA XML. However, we do not require model data to adhere strictly to the OpenPRA JSON layout; each grammar or file type can continue to use its own native parsing and serialization.

% As shown in Figure~\ref{fig:translations}, this setup allows direct "bridges" among formats. Tools like FTREX FTP or SAPHIRE JSON remain parsed by their existing libraries, then converted into others on demand without mandating a monolithic pivot. For instance, a MAR-D representation can be validated by its native routines and output as OpenPSA XML or even transformed into TensorFlow Graph form. Likewise, CAFTA XML may flow directly into FTREX FTP or SAPHSOLVE JSON. Overall, the architecture constructs a mesh of translators leveraging format-specific toolkits yet avoids imposing a unitary, intermediate data structure.

% A major complicating factor in tool development in the PRA domain is model representation and exchange. As a direct consequence of closed-sourced development practices, format/syntax fragmentation is a non-trivial problem. At the same time, the number of stakeholders in the PRA field is also quite limited. All this to say, for such a niche and already fragmented field, creating a *new* format (with the purehearted intent of unifying all others) actually tends to have the opposite effect.

% Alternatively, creating interoperability between every existing format is equally ambitious, in part because there are so many formats, but also because there is no incentive in a mostly closed development community to invest effort in improving interoperability. As developers with limited time and other important priorities, we have chosen a somewhat unique but nonetheless manageable strategy to improve model exchange/interoperability (whether this strategy pays off, is ultimately dependent on community buy-in). The first step (and short term goal) here is to develop simple interfaces for loosely coupling existing translation tools. We do the heavy lifting of maximizing translatability by building a maximally traversable graph. Our objective, so to speak, is to facilitate the development of a "translation spanning tree", as opposed to an ambitious fully-connected translation map. This will also us to leverage existing translators for verification tasks using round-trip testing and related concepts.

% The primary intent behind the model translation functionality is to enable each format or grammar to be converted into another without forcing all data through a single, unified intermediate representation. As illustrated in Figure~\ref{fig:translations}, each tool or library can be reused in its native domain. For example, using existing functions to read FTREX FTP or SAPHIRE JSON, then translating directly between formats as needed. This approach removes the burden of maintaining a monolithic "pivot" structure. Instead, each format retains its standard parsing and exporting routines, while the translation system orchestrates coherent data exchange among them.

% In practice, this architecture supports a diverse range of flows: for instance, a MAR-D representation can be read and validated through specialized libraries, then published as OpenPSA XML or further transformed into a TensorFlow Graph. Similarly, a CAFTA XML project can pass directly into FTREX FTP or SAPHSOLVE JSON, depending on user goals. The main result is a flexible "mesh" of supported transformations that exploits existing, format-specific libraries without requiring additional, centralized data structures or forcing all conversions through a single point.

% The second step (and long-term goal) is to indeed develop our own open-source schema with extremely open licensing (this is the OpenPRA JSON Schema). This is in part because OpenPSA XML is no longer under development. While we could maintain and extend OpenPSA XML, there are atleast two reasons for introducing our own schema in JSON. One, we believe JSON is more extensible, adoptable, and ultimately more user-friendly. For example, like ML models (often stored as flatbuffers), PRA models are built to be very large, but unlike ML models, parameters are ultimately not merely arbitrary weights. Rather, they are a knowledge representation, and therefore need to be human readable (something XML does well) while also being parser-friendly (JSON Record or sorted arrays improve lookup times). Second, OpenPSA semantics are generic to probabilistic safety (which is great), but we intend to primarily serve the nuclear licensing and risk community. So, our objective is to develop OpenPRA JSON to maintain conceptual parity with OpenPSA XML, and then extend it (within a nuclear namespace) to serve our licensing needs. We have already implemented the OpenPSA stochastic and event tree, fault tree layers in OpenPRA JSON, and work on extending it to meet nuclear licensing needs is already under way (checkout this link https://openpra-org.github.io/openpra-monorepo/index.html)

% \input{3_identifying_gaps/benchmarking/datasets/translation/saphsolve_openpsa}